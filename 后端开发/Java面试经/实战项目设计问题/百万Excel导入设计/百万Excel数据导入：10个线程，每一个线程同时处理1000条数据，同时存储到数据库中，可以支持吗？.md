## 背景介绍

我们要导入20个Sheet，每一个Sheet下是60000条数据，总数据大概是1200000条，我们开启10个线程，每一个线程单批次处理量是1000条。

也就是说：单个线程处理1000条，10个线程大概同时进行，会同时往MySQL数据库中存储10000条数据。

从 MySQL 数据库本身的角度来看，导入数据的并发处理可能会带来一些问题。以下是分析和解决方案：

### 1. **问题分析：**

#### 1.1 并发写入压力

每个线程处理 1000 条数据，10 个线程同时写入 MySQL，这意味着每秒会有多个并发的插入/更新操作。如果没有合理的控制和优化，可能会遇到以下问题：   

- **锁争用问题**：如果多个线程同时插入相同的表，尤其是带有索引的表，可能会导致行锁或表锁，影响性能。
- **事务冲突**：如果插入数据时涉及事务管理，多个并发事务可能会产生锁定冲突（如死锁或阻塞），导致性能下降。
- **写入性能瓶颈**：单表大量写入数据，尤其是带有外键、触发器等约束时，可能导致 MySQL 写入性能下降。
- **IO** **瓶颈**：大量写入操作可能会给 MySQL 的磁盘 I/O 带来较大的压力，尤其是在存储引擎为 InnoDB 时，因为其需要持久化数据和日志。

#### 1.2 数据库连接数限制

10 个线程同时访问 MySQL，意味着会有至少 10 个数据库连接。如果连接池大小设置不合理，可能会导致连接池枯竭或 MySQL 拒绝新连接。

#### 1.3 数据库事务和批处理效率

每个线程处理 1000 条数据，如果每条数据单独进行插入操作，MySQL 的事务提交会频繁发生，这会导致性能大幅下降。每个插入操作都会触发磁盘 I/O 和日志写入。

#### 1.4 索引更新和数据一致性

表中如果存在大量索引，批量插入时每一条数据的插入都会触发索引的更新，这会降低写入性能。同时需要注意在并发情况下的数据一致性问题。

### 2. **解决方案：**

#### 2.1 批量插入优化

可以使用 MySQL 的批量插入功能，一次性提交 1000 条数据，而不是逐条插入。可以使用 `INSERT INTO ... VALUES (...)` 语法，或者 MyBatis 的批处理功能来提高插入效率。

```SQL
INSERT INTO your_table (col1, col2, col3) VALUES 
    (val1, val2, val3), 
    (val4, val5, val6), 
    ... 
    (val1000, val1001, val1002);
```

这可以显著减少事务提交的频率，降低 I/O 开销。

#### 2.2 调整事务提交策略

使用 MyBatis 等框架时，确保每批次数据在一个事务内处理。可以通过手动管理事务，避免每条数据都触发事务提交。

- 在业务逻辑层面开启事务，处理完一批数据后再提交，避免频繁的事务开销。
- 调整 `autocommit`，确保事务是在批量完成后再提交。

#### 2.3 使用数据库连接池并合理配置

根据 10 个线程并发的需求，调整数据库连接池（如 Druid、HikariCP）的最大连接数配置。确保有足够的连接可以分配给并发的线程。一般可以根据并发数设置适当的连接数，比如设置为 `10-20`。

```YAML
spring.datasource.hikari.maximum-pool-size: 20
```

#### 2.4 表结构和索引优化

- **合理使用索引**：尽量减少不必要的索引。批量插入时，如果索引过多，可能导致写入速度慢。可以在插入大量数据时临时禁用索引，插入完成后再重新启用。
- **主键****设计**：如果是自增主键，高并发写入时自增可能成为瓶颈，建议使用 UUID 或者雪花算法生成唯一主键，减少锁竞争。

#### 2.5 调整 MySQL 配置

- **innodb_flush_log_at_trx_commit**：可以将该参数设置为 2，在每个事务提交时不会立即将日志同步到磁盘，而是每秒刷盘一次，能够提高写入性能。

```SQL
SET GLOBAL innodb_flush_log_at_trx_commit = 2;
```

- **innodb_buffer_pool_size**：增加 `innodb_buffer_pool_size` 的大小，确保足够的内存用于缓存数据和索引，减少磁盘 I/O。
- **innodb_lock_wait_timeout**：适当增加锁等待时间，减少因锁冲突导致的错误。

#### 2.6 数据分区（Sharding）

如果 120 万条数据分布在一张表中，可以考虑将表按日期或其他字段进行分区（Partition）处理，减少单表的写入压力。

#### 2.7 使用消息队列（如 RocketMQ）进行异步处理

如果导入操作不需要实时性，可以将数据通过消息队列异步写入 MySQL，减少数据库瞬时并发写入的压力。

### 总结：

- 采用批量插入，减少每个事务的提交次数，避免频繁的磁盘 I/O。
- 合理配置数据库连接池，确保并发连接的稳定性。
- 优化数据库的索引结构，必要时调整 MySQL 配置，如调整 `innodb_flush_log_at_trx_commit` 和 `innodb_buffer_pool_size`。
- 如果并发写入压力过大，可以使用消息队列异步写入，或者进行数据分区，减少单表压力。

通过这些优化措施，MySQL 应该可以处理这种规模的数据导入。